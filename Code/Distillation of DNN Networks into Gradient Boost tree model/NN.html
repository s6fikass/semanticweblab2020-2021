<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN">
<html><head><title>Python: module NN</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
</head><body bgcolor="#f0f0f8">

<table width="100%" cellspacing=0 cellpadding=2 border=0 summary="heading">
<tr bgcolor="#7799ee">
<td valign=bottom>&nbsp;<br>
<font color="#ffffff" face="helvetica, arial">&nbsp;<br><big><big><strong>NN</strong></big></big></font></td
><td align=right valign=bottom
><font color="#ffffff" face="helvetica, arial"><a href=".">index</a><br><a href="file:e%3A%5Cmyproject%5Cws_20_21%5Cknowledgedistillation%5Cnn.py">e:\myproject\ws_20_21\knowledgedistillation\nn.py</a></font></td></tr></table>
    <p></p>
<p>
<table width="100%" cellspacing=0 cellpadding=2 border=0 summary="section">
<tr bgcolor="#aa55cc">
<td colspan=3 valign=bottom>&nbsp;<br>
<font color="#ffffff" face="helvetica, arial"><big><strong>Modules</strong></big></font></td></tr>
    
<tr><td bgcolor="#aa55cc"><tt>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</tt></td><td>&nbsp;</td>
<td width="100%"><table width="100%" summary="list"><tr><td width="25%" valign=top><a href="numpy.html">numpy</a><br>
</td><td width="25%" valign=top><a href="matplotlib.pyplot.html">matplotlib.pyplot</a><br>
</td><td width="25%" valign=top></td><td width="25%" valign=top></td></tr></table></td></tr></table><p>
<table width="100%" cellspacing=0 cellpadding=2 border=0 summary="section">
<tr bgcolor="#ee77aa">
<td colspan=3 valign=bottom>&nbsp;<br>
<font color="#ffffff" face="helvetica, arial"><big><strong>Classes</strong></big></font></td></tr>
    
<tr><td bgcolor="#ee77aa"><tt>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</tt></td><td>&nbsp;</td>
<td width="100%"><dl>
<dt><font face="helvetica, arial"><a href="torch.nn.modules.module.html#Module">torch.nn.modules.module.Module</a>(<a href="builtins.html#object">builtins.object</a>)
</font></dt><dd>
<dl>
<dt><font face="helvetica, arial"><a href="NN.html#MLP">MLP</a>
</font></dt></dl>
</dd>
</dl>
 <p>
<table width="100%" cellspacing=0 cellpadding=2 border=0 summary="section">
<tr bgcolor="#ffc8d8">
<td colspan=3 valign=bottom>&nbsp;<br>
<font color="#000000" face="helvetica, arial"><a name="MLP">class <strong>MLP</strong></a>(<a href="torch.nn.modules.module.html#Module">torch.nn.modules.module.Module</a>)</font></td></tr>
    
<tr bgcolor="#ffc8d8"><td rowspan=2><tt>&nbsp;&nbsp;&nbsp;</tt></td>
<td colspan=2><tt><a href="#MLP">MLP</a>(n_inputs)<br>
&nbsp;<br>
Define&nbsp;a&nbsp;class&nbsp;that&nbsp;extends&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>&nbsp;class.<br>&nbsp;</tt></td></tr>
<tr><td>&nbsp;</td>
<td width="100%"><dl><dt>Method resolution order:</dt>
<dd><a href="NN.html#MLP">MLP</a></dd>
<dd><a href="torch.nn.modules.module.html#Module">torch.nn.modules.module.Module</a></dd>
<dd><a href="builtins.html#object">builtins.object</a></dd>
</dl>
<hr>
Methods defined here:<br>
<dl><dt><a name="MLP-__init__"><strong>__init__</strong></a>(self, n_inputs)</dt><dd><tt>The&nbsp;constructor&nbsp;of&nbsp;<a href="#MLP">MLP</a>&nbsp;class&nbsp;defines&nbsp;the&nbsp;layers.&nbsp;We&nbsp;define&nbsp;<a href="#MLP">MLP</a><br>
with&nbsp;input&nbsp;layer(n_inputs&nbsp;=&nbsp;13&nbsp;neurons),&nbsp;first&nbsp;hidden&nbsp;layer(26&nbsp;neurons),<br>
second&nbsp;hidden&nbsp;layer(13&nbsp;neurons)&nbsp;and&nbsp;output&nbsp;layer.&nbsp;That&nbsp;gives&nbsp;us&nbsp;689<br>
weights&nbsp;to&nbsp;be&nbsp;adjusted.&nbsp;We&nbsp;are&nbsp;using&nbsp;Kaiming&nbsp;for&nbsp;the&nbsp;weight&nbsp;initialisation<br>
strategy&nbsp;for&nbsp;hidden1&nbsp;-&nbsp;&gt;&nbsp;hidden2,&nbsp;hidden2&nbsp;-&nbsp;&gt;&nbsp;output&nbsp;layer,&nbsp;according&nbsp;to&nbsp;the<br>
fact&nbsp;that&nbsp;we&nbsp;are&nbsp;using&nbsp;ReLU()&nbsp;activation&nbsp;function&nbsp;for&nbsp;the&nbsp;both&nbsp;hidden&nbsp;layers.<br>
For&nbsp;the&nbsp;output&nbsp;layer,&nbsp;we&nbsp;are&nbsp;using&nbsp;Sigmoid()&nbsp;activation&nbsp;function,&nbsp;suitable<br>
for&nbsp;our&nbsp;binary&nbsp;classification&nbsp;task.&nbsp;We&nbsp;are&nbsp;using&nbsp;Xavier&nbsp;initialization&nbsp;for<br>
the&nbsp;weights&nbsp;from&nbsp;hidden2&nbsp;-&gt;&nbsp;output,&nbsp;because&nbsp;it&nbsp;can&nbsp;solve&nbsp;Sigmoid()&nbsp;vanishing<br>
gradient&nbsp;problem.</tt></dd></dl>

<dl><dt><a name="MLP-forward"><strong>forward</strong></a>(self, X)</dt><dd><tt>This&nbsp;function&nbsp;takes&nbsp;the&nbsp;input&nbsp;data&nbsp;(rows&nbsp;of&nbsp;heart.csv,&nbsp;without&nbsp;original<br>
target&nbsp;labels).&nbsp;The&nbsp;input&nbsp;data&nbsp;is&nbsp;fed&nbsp;in&nbsp;the&nbsp;forward&nbsp;direction&nbsp;through<br>
the&nbsp;network.&nbsp;Each&nbsp;hidden&nbsp;layer&nbsp;accepts&nbsp;the&nbsp;input&nbsp;data,&nbsp;processes&nbsp;it,&nbsp;as&nbsp;per<br>
the&nbsp;activation&nbsp;function&nbsp;and&nbsp;passes&nbsp;to&nbsp;the&nbsp;successive&nbsp;layer.<br>
&nbsp;<br>
Parameters<br>
----------<br>
X&nbsp;:&nbsp;input&nbsp;values&nbsp;from&nbsp;heart.csv&nbsp;dataset&nbsp;(without&nbsp;the&nbsp;last&nbsp;"target&nbsp;column")<br>
&nbsp;<br>
Returns<br>
-------<br>
X&nbsp;:&nbsp;calculated&nbsp;output(target)&nbsp;value&nbsp;for&nbsp;the&nbsp;given&nbsp;input</tt></dd></dl>

<dl><dt><a name="MLP-forwardLastHiddenLayer"><strong>forwardLastHiddenLayer</strong></a>(self, X)</dt><dd><tt>This&nbsp;function&nbsp;takes&nbsp;the&nbsp;input&nbsp;data&nbsp;(rows&nbsp;of&nbsp;heart.csv,&nbsp;without&nbsp;original<br>
target&nbsp;labels).&nbsp;The&nbsp;input&nbsp;data&nbsp;is&nbsp;fed&nbsp;in&nbsp;the&nbsp;forward&nbsp;direction&nbsp;through<br>
the&nbsp;network.&nbsp;Each&nbsp;hidden&nbsp;layer&nbsp;accepts&nbsp;the&nbsp;input&nbsp;data,&nbsp;processes&nbsp;it,&nbsp;as&nbsp;per<br>
the&nbsp;activation&nbsp;function&nbsp;and&nbsp;passes&nbsp;to&nbsp;the&nbsp;successive&nbsp;layer.&nbsp;This&nbsp;function<br>
returns&nbsp;the&nbsp;output&nbsp;of&nbsp;the&nbsp;second&nbsp;hidden&nbsp;layer.&nbsp;The&nbsp;goal&nbsp;is&nbsp;to&nbsp;extract&nbsp;the<br>
learned&nbsp;features&nbsp;from&nbsp;the&nbsp;last&nbsp;hidden&nbsp;layer&nbsp;(in&nbsp;our&nbsp;case&nbsp;-&nbsp;second&nbsp;hidden&nbsp;layer)<br>
&nbsp;<br>
Parameters<br>
----------<br>
X&nbsp;:&nbsp;input&nbsp;values&nbsp;from&nbsp;heart.csv&nbsp;dataset&nbsp;(without&nbsp;the&nbsp;last&nbsp;"target&nbsp;column")<br>
&nbsp;<br>
Returns<br>
-------<br>
X&nbsp;:&nbsp;learned&nbsp;features&nbsp;from&nbsp;the&nbsp;second&nbsp;hidden&nbsp;layer</tt></dd></dl>

<hr>
Methods inherited from <a href="torch.nn.modules.module.html#Module">torch.nn.modules.module.Module</a>:<br>
<dl><dt><a name="MLP-__call__"><strong>__call__</strong></a> = <a href="#MLP-_call_impl">_call_impl</a>(self, *input, **kwargs)</dt></dl>

<dl><dt><a name="MLP-__delattr__"><strong>__delattr__</strong></a>(self, name)</dt><dd><tt>Implement&nbsp;delattr(self,&nbsp;name).</tt></dd></dl>

<dl><dt><a name="MLP-__dir__"><strong>__dir__</strong></a>(self)</dt><dd><tt>Default&nbsp;dir()&nbsp;implementation.</tt></dd></dl>

<dl><dt><a name="MLP-__getattr__"><strong>__getattr__</strong></a>(self, name: str) -&gt; Union[torch.Tensor, ForwardRef('Module')]</dt></dl>

<dl><dt><a name="MLP-__repr__"><strong>__repr__</strong></a>(self)</dt><dd><tt>Return&nbsp;repr(self).</tt></dd></dl>

<dl><dt><a name="MLP-__setattr__"><strong>__setattr__</strong></a>(self, name: str, value: Union[torch.Tensor, ForwardRef('Module')]) -&gt; None</dt><dd><tt>Implement&nbsp;setattr(self,&nbsp;name,&nbsp;value).</tt></dd></dl>

<dl><dt><a name="MLP-__setstate__"><strong>__setstate__</strong></a>(self, state)</dt></dl>

<dl><dt><a name="MLP-add_module"><strong>add_module</strong></a>(self, name: str, module: Union[ForwardRef('Module'), NoneType]) -&gt; None</dt><dd><tt>Adds&nbsp;a&nbsp;child&nbsp;module&nbsp;to&nbsp;the&nbsp;current&nbsp;module.<br>
&nbsp;<br>
The&nbsp;module&nbsp;can&nbsp;be&nbsp;accessed&nbsp;as&nbsp;an&nbsp;attribute&nbsp;using&nbsp;the&nbsp;given&nbsp;name.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;name&nbsp;(string):&nbsp;name&nbsp;of&nbsp;the&nbsp;child&nbsp;module.&nbsp;The&nbsp;child&nbsp;module&nbsp;can&nbsp;be<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;accessed&nbsp;from&nbsp;this&nbsp;module&nbsp;using&nbsp;the&nbsp;given&nbsp;name<br>
&nbsp;&nbsp;&nbsp;&nbsp;module&nbsp;(<a href="torch.nn.modules.module.html#Module">Module</a>):&nbsp;child&nbsp;module&nbsp;to&nbsp;be&nbsp;added&nbsp;to&nbsp;the&nbsp;module.</tt></dd></dl>

<dl><dt><a name="MLP-apply"><strong>apply</strong></a>(self: ~T, fn: Callable[[ForwardRef('Module')], NoneType]) -&gt; ~T</dt><dd><tt>Applies&nbsp;``fn``&nbsp;recursively&nbsp;to&nbsp;every&nbsp;submodule&nbsp;(as&nbsp;returned&nbsp;by&nbsp;``.<a href="#MLP-children">children</a>()``)<br>
as&nbsp;well&nbsp;as&nbsp;self.&nbsp;Typical&nbsp;use&nbsp;includes&nbsp;initializing&nbsp;the&nbsp;parameters&nbsp;of&nbsp;a&nbsp;model<br>
(see&nbsp;also&nbsp;:ref:`nn-init-doc`).<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;fn&nbsp;(:class:`<a href="torch.nn.modules.module.html#Module">Module</a>`&nbsp;-&gt;&nbsp;None):&nbsp;function&nbsp;to&nbsp;be&nbsp;applied&nbsp;to&nbsp;each&nbsp;submodule<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self<br>
&nbsp;<br>
Example::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;@torch.no_grad()<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;def&nbsp;init_weights(m):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(m)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;<a href="#MLP-type">type</a>(m)&nbsp;==&nbsp;nn.Linear:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;m.weight.fill_(1.0)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(m.weight)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;net&nbsp;=&nbsp;nn.Sequential(nn.Linear(2,&nbsp;2),&nbsp;nn.Linear(2,&nbsp;2))<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;net.<a href="#MLP-apply">apply</a>(init_weights)<br>
&nbsp;&nbsp;&nbsp;&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;Parameter&nbsp;containing:<br>
&nbsp;&nbsp;&nbsp;&nbsp;tensor([[&nbsp;1.,&nbsp;&nbsp;1.],<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[&nbsp;1.,&nbsp;&nbsp;1.]])<br>
&nbsp;&nbsp;&nbsp;&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;Parameter&nbsp;containing:<br>
&nbsp;&nbsp;&nbsp;&nbsp;tensor([[&nbsp;1.,&nbsp;&nbsp;1.],<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[&nbsp;1.,&nbsp;&nbsp;1.]])<br>
&nbsp;&nbsp;&nbsp;&nbsp;Sequential(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(0):&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(1):&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;)<br>
&nbsp;&nbsp;&nbsp;&nbsp;Sequential(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(0):&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(1):&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;)</tt></dd></dl>

<dl><dt><a name="MLP-bfloat16"><strong>bfloat16</strong></a>(self: ~T) -&gt; ~T</dt><dd><tt>Casts&nbsp;all&nbsp;floating&nbsp;point&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;to&nbsp;``bfloat16``&nbsp;datatype.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</tt></dd></dl>

<dl><dt><a name="MLP-buffers"><strong>buffers</strong></a>(self, recurse: bool = True) -&gt; Iterator[torch.Tensor]</dt><dd><tt>Returns&nbsp;an&nbsp;iterator&nbsp;over&nbsp;module&nbsp;buffers.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;recurse&nbsp;(bool):&nbsp;if&nbsp;True,&nbsp;then&nbsp;yields&nbsp;buffers&nbsp;of&nbsp;this&nbsp;module<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;and&nbsp;all&nbsp;submodules.&nbsp;Otherwise,&nbsp;yields&nbsp;only&nbsp;buffers&nbsp;that<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;are&nbsp;direct&nbsp;members&nbsp;of&nbsp;this&nbsp;module.<br>
&nbsp;<br>
Yields:<br>
&nbsp;&nbsp;&nbsp;&nbsp;torch.Tensor:&nbsp;module&nbsp;buffer<br>
&nbsp;<br>
Example::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;for&nbsp;buf&nbsp;in&nbsp;model.<a href="#MLP-buffers">buffers</a>():<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(<a href="#MLP-type">type</a>(buf),&nbsp;buf.size())<br>
&nbsp;&nbsp;&nbsp;&nbsp;&lt;class&nbsp;'torch.Tensor'&gt;&nbsp;(20L,)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&lt;class&nbsp;'torch.Tensor'&gt;&nbsp;(20L,&nbsp;1L,&nbsp;5L,&nbsp;5L)</tt></dd></dl>

<dl><dt><a name="MLP-children"><strong>children</strong></a>(self) -&gt; Iterator[ForwardRef('Module')]</dt><dd><tt>Returns&nbsp;an&nbsp;iterator&nbsp;over&nbsp;immediate&nbsp;children&nbsp;modules.<br>
&nbsp;<br>
Yields:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;a&nbsp;child&nbsp;module</tt></dd></dl>

<dl><dt><a name="MLP-cpu"><strong>cpu</strong></a>(self: ~T) -&gt; ~T</dt><dd><tt>Moves&nbsp;all&nbsp;model&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;to&nbsp;the&nbsp;CPU.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</tt></dd></dl>

<dl><dt><a name="MLP-cuda"><strong>cuda</strong></a>(self: ~T, device: Union[int, torch.device, NoneType] = None) -&gt; ~T</dt><dd><tt>Moves&nbsp;all&nbsp;model&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;to&nbsp;the&nbsp;GPU.<br>
&nbsp;<br>
This&nbsp;also&nbsp;makes&nbsp;associated&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;different&nbsp;objects.&nbsp;So<br>
it&nbsp;should&nbsp;be&nbsp;called&nbsp;before&nbsp;constructing&nbsp;optimizer&nbsp;if&nbsp;the&nbsp;module&nbsp;will<br>
live&nbsp;on&nbsp;GPU&nbsp;while&nbsp;being&nbsp;optimized.<br>
&nbsp;<br>
Arguments:<br>
&nbsp;&nbsp;&nbsp;&nbsp;device&nbsp;(int,&nbsp;optional):&nbsp;if&nbsp;specified,&nbsp;all&nbsp;parameters&nbsp;will&nbsp;be<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;copied&nbsp;to&nbsp;that&nbsp;device<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</tt></dd></dl>

<dl><dt><a name="MLP-double"><strong>double</strong></a>(self: ~T) -&gt; ~T</dt><dd><tt>Casts&nbsp;all&nbsp;floating&nbsp;point&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;to&nbsp;``double``&nbsp;datatype.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</tt></dd></dl>

<dl><dt><a name="MLP-eval"><strong>eval</strong></a>(self: ~T) -&gt; ~T</dt><dd><tt>Sets&nbsp;the&nbsp;module&nbsp;in&nbsp;evaluation&nbsp;mode.<br>
&nbsp;<br>
This&nbsp;has&nbsp;any&nbsp;effect&nbsp;only&nbsp;on&nbsp;certain&nbsp;modules.&nbsp;See&nbsp;documentations&nbsp;of<br>
particular&nbsp;modules&nbsp;for&nbsp;details&nbsp;of&nbsp;their&nbsp;behaviors&nbsp;in&nbsp;training/evaluation<br>
mode,&nbsp;if&nbsp;they&nbsp;are&nbsp;affected,&nbsp;e.g.&nbsp;:class:`Dropout`,&nbsp;:class:`BatchNorm`,<br>
etc.<br>
&nbsp;<br>
This&nbsp;is&nbsp;equivalent&nbsp;with&nbsp;:meth:`self.<a href="#MLP-train">train</a>(False)&nbsp;&lt;torch.nn.<a href="torch.nn.modules.module.html#Module">Module</a>.train&gt;`.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</tt></dd></dl>

<dl><dt><a name="MLP-extra_repr"><strong>extra_repr</strong></a>(self) -&gt; str</dt><dd><tt>Set&nbsp;the&nbsp;extra&nbsp;representation&nbsp;of&nbsp;the&nbsp;module<br>
&nbsp;<br>
To&nbsp;print&nbsp;customized&nbsp;extra&nbsp;information,&nbsp;you&nbsp;should&nbsp;re-implement<br>
this&nbsp;method&nbsp;in&nbsp;your&nbsp;own&nbsp;modules.&nbsp;Both&nbsp;single-line&nbsp;and&nbsp;multi-line<br>
strings&nbsp;are&nbsp;acceptable.</tt></dd></dl>

<dl><dt><a name="MLP-float"><strong>float</strong></a>(self: ~T) -&gt; ~T</dt><dd><tt>Casts&nbsp;all&nbsp;floating&nbsp;point&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;to&nbsp;float&nbsp;datatype.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</tt></dd></dl>

<dl><dt><a name="MLP-half"><strong>half</strong></a>(self: ~T) -&gt; ~T</dt><dd><tt>Casts&nbsp;all&nbsp;floating&nbsp;point&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;to&nbsp;``half``&nbsp;datatype.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</tt></dd></dl>

<dl><dt><a name="MLP-load_state_dict"><strong>load_state_dict</strong></a>(self, state_dict: Dict[str, torch.Tensor], strict: bool = True)</dt><dd><tt>Copies&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;from&nbsp;:attr:`state_dict`&nbsp;into<br>
this&nbsp;module&nbsp;and&nbsp;its&nbsp;descendants.&nbsp;If&nbsp;:attr:`strict`&nbsp;is&nbsp;``True``,&nbsp;then<br>
the&nbsp;keys&nbsp;of&nbsp;:attr:`state_dict`&nbsp;must&nbsp;exactly&nbsp;match&nbsp;the&nbsp;keys&nbsp;returned<br>
by&nbsp;this&nbsp;module's&nbsp;:meth:`~torch.nn.<a href="torch.nn.modules.module.html#Module">Module</a>.state_dict`&nbsp;function.<br>
&nbsp;<br>
Arguments:<br>
&nbsp;&nbsp;&nbsp;&nbsp;state_dict&nbsp;(dict):&nbsp;a&nbsp;dict&nbsp;containing&nbsp;parameters&nbsp;and<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;persistent&nbsp;buffers.<br>
&nbsp;&nbsp;&nbsp;&nbsp;strict&nbsp;(bool,&nbsp;optional):&nbsp;whether&nbsp;to&nbsp;strictly&nbsp;enforce&nbsp;that&nbsp;the&nbsp;keys<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;in&nbsp;:attr:`state_dict`&nbsp;match&nbsp;the&nbsp;keys&nbsp;returned&nbsp;by&nbsp;this&nbsp;module's<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;:meth:`~torch.nn.<a href="torch.nn.modules.module.html#Module">Module</a>.state_dict`&nbsp;function.&nbsp;Default:&nbsp;``True``<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;``NamedTuple``&nbsp;with&nbsp;``missing_keys``&nbsp;and&nbsp;``unexpected_keys``&nbsp;fields:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;*&nbsp;**missing_keys**&nbsp;is&nbsp;a&nbsp;list&nbsp;of&nbsp;str&nbsp;containing&nbsp;the&nbsp;missing&nbsp;keys<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;*&nbsp;**unexpected_keys**&nbsp;is&nbsp;a&nbsp;list&nbsp;of&nbsp;str&nbsp;containing&nbsp;the&nbsp;unexpected&nbsp;keys</tt></dd></dl>

<dl><dt><a name="MLP-modules"><strong>modules</strong></a>(self) -&gt; Iterator[ForwardRef('Module')]</dt><dd><tt>Returns&nbsp;an&nbsp;iterator&nbsp;over&nbsp;all&nbsp;modules&nbsp;in&nbsp;the&nbsp;network.<br>
&nbsp;<br>
Yields:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;a&nbsp;module&nbsp;in&nbsp;the&nbsp;network<br>
&nbsp;<br>
Note:<br>
&nbsp;&nbsp;&nbsp;&nbsp;Duplicate&nbsp;modules&nbsp;are&nbsp;returned&nbsp;only&nbsp;once.&nbsp;In&nbsp;the&nbsp;following<br>
&nbsp;&nbsp;&nbsp;&nbsp;example,&nbsp;``l``&nbsp;will&nbsp;be&nbsp;returned&nbsp;only&nbsp;once.<br>
&nbsp;<br>
Example::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;l&nbsp;=&nbsp;nn.Linear(2,&nbsp;2)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;net&nbsp;=&nbsp;nn.Sequential(l,&nbsp;l)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;for&nbsp;idx,&nbsp;m&nbsp;in&nbsp;enumerate(net.<a href="#MLP-modules">modules</a>()):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(idx,&nbsp;'-&gt;',&nbsp;m)<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;0&nbsp;-&gt;&nbsp;Sequential(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(0):&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(1):&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;)<br>
&nbsp;&nbsp;&nbsp;&nbsp;1&nbsp;-&gt;&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)</tt></dd></dl>

<dl><dt><a name="MLP-named_buffers"><strong>named_buffers</strong></a>(self, prefix: str = '', recurse: bool = True) -&gt; Iterator[Tuple[str, torch.Tensor]]</dt><dd><tt>Returns&nbsp;an&nbsp;iterator&nbsp;over&nbsp;module&nbsp;buffers,&nbsp;yielding&nbsp;both&nbsp;the<br>
name&nbsp;of&nbsp;the&nbsp;buffer&nbsp;as&nbsp;well&nbsp;as&nbsp;the&nbsp;buffer&nbsp;itself.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;prefix&nbsp;(str):&nbsp;prefix&nbsp;to&nbsp;prepend&nbsp;to&nbsp;all&nbsp;buffer&nbsp;names.<br>
&nbsp;&nbsp;&nbsp;&nbsp;recurse&nbsp;(bool):&nbsp;if&nbsp;True,&nbsp;then&nbsp;yields&nbsp;buffers&nbsp;of&nbsp;this&nbsp;module<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;and&nbsp;all&nbsp;submodules.&nbsp;Otherwise,&nbsp;yields&nbsp;only&nbsp;buffers&nbsp;that<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;are&nbsp;direct&nbsp;members&nbsp;of&nbsp;this&nbsp;module.<br>
&nbsp;<br>
Yields:<br>
&nbsp;&nbsp;&nbsp;&nbsp;(string,&nbsp;torch.Tensor):&nbsp;Tuple&nbsp;containing&nbsp;the&nbsp;name&nbsp;and&nbsp;buffer<br>
&nbsp;<br>
Example::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;for&nbsp;name,&nbsp;buf&nbsp;in&nbsp;self.<a href="#MLP-named_buffers">named_buffers</a>():<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;name&nbsp;in&nbsp;['running_var']:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(buf.size())</tt></dd></dl>

<dl><dt><a name="MLP-named_children"><strong>named_children</strong></a>(self) -&gt; Iterator[Tuple[str, ForwardRef('Module')]]</dt><dd><tt>Returns&nbsp;an&nbsp;iterator&nbsp;over&nbsp;immediate&nbsp;children&nbsp;modules,&nbsp;yielding&nbsp;both<br>
the&nbsp;name&nbsp;of&nbsp;the&nbsp;module&nbsp;as&nbsp;well&nbsp;as&nbsp;the&nbsp;module&nbsp;itself.<br>
&nbsp;<br>
Yields:<br>
&nbsp;&nbsp;&nbsp;&nbsp;(string,&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>):&nbsp;Tuple&nbsp;containing&nbsp;a&nbsp;name&nbsp;and&nbsp;child&nbsp;module<br>
&nbsp;<br>
Example::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;for&nbsp;name,&nbsp;module&nbsp;in&nbsp;model.<a href="#MLP-named_children">named_children</a>():<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;name&nbsp;in&nbsp;['conv4',&nbsp;'conv5']:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(module)</tt></dd></dl>

<dl><dt><a name="MLP-named_modules"><strong>named_modules</strong></a>(self, memo: Union[Set[ForwardRef('Module')], NoneType] = None, prefix: str = '')</dt><dd><tt>Returns&nbsp;an&nbsp;iterator&nbsp;over&nbsp;all&nbsp;modules&nbsp;in&nbsp;the&nbsp;network,&nbsp;yielding<br>
both&nbsp;the&nbsp;name&nbsp;of&nbsp;the&nbsp;module&nbsp;as&nbsp;well&nbsp;as&nbsp;the&nbsp;module&nbsp;itself.<br>
&nbsp;<br>
Yields:<br>
&nbsp;&nbsp;&nbsp;&nbsp;(string,&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>):&nbsp;Tuple&nbsp;of&nbsp;name&nbsp;and&nbsp;module<br>
&nbsp;<br>
Note:<br>
&nbsp;&nbsp;&nbsp;&nbsp;Duplicate&nbsp;modules&nbsp;are&nbsp;returned&nbsp;only&nbsp;once.&nbsp;In&nbsp;the&nbsp;following<br>
&nbsp;&nbsp;&nbsp;&nbsp;example,&nbsp;``l``&nbsp;will&nbsp;be&nbsp;returned&nbsp;only&nbsp;once.<br>
&nbsp;<br>
Example::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;l&nbsp;=&nbsp;nn.Linear(2,&nbsp;2)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;net&nbsp;=&nbsp;nn.Sequential(l,&nbsp;l)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;for&nbsp;idx,&nbsp;m&nbsp;in&nbsp;enumerate(net.<a href="#MLP-named_modules">named_modules</a>()):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(idx,&nbsp;'-&gt;',&nbsp;m)<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;0&nbsp;-&gt;&nbsp;('',&nbsp;Sequential(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(0):&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(1):&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;))<br>
&nbsp;&nbsp;&nbsp;&nbsp;1&nbsp;-&gt;&nbsp;('0',&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True))</tt></dd></dl>

<dl><dt><a name="MLP-named_parameters"><strong>named_parameters</strong></a>(self, prefix: str = '', recurse: bool = True) -&gt; Iterator[Tuple[str, torch.Tensor]]</dt><dd><tt>Returns&nbsp;an&nbsp;iterator&nbsp;over&nbsp;module&nbsp;parameters,&nbsp;yielding&nbsp;both&nbsp;the<br>
name&nbsp;of&nbsp;the&nbsp;parameter&nbsp;as&nbsp;well&nbsp;as&nbsp;the&nbsp;parameter&nbsp;itself.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;prefix&nbsp;(str):&nbsp;prefix&nbsp;to&nbsp;prepend&nbsp;to&nbsp;all&nbsp;parameter&nbsp;names.<br>
&nbsp;&nbsp;&nbsp;&nbsp;recurse&nbsp;(bool):&nbsp;if&nbsp;True,&nbsp;then&nbsp;yields&nbsp;parameters&nbsp;of&nbsp;this&nbsp;module<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;and&nbsp;all&nbsp;submodules.&nbsp;Otherwise,&nbsp;yields&nbsp;only&nbsp;parameters&nbsp;that<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;are&nbsp;direct&nbsp;members&nbsp;of&nbsp;this&nbsp;module.<br>
&nbsp;<br>
Yields:<br>
&nbsp;&nbsp;&nbsp;&nbsp;(string,&nbsp;Parameter):&nbsp;Tuple&nbsp;containing&nbsp;the&nbsp;name&nbsp;and&nbsp;parameter<br>
&nbsp;<br>
Example::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;for&nbsp;name,&nbsp;param&nbsp;in&nbsp;self.<a href="#MLP-named_parameters">named_parameters</a>():<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;name&nbsp;in&nbsp;['bias']:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(param.size())</tt></dd></dl>

<dl><dt><a name="MLP-parameters"><strong>parameters</strong></a>(self, recurse: bool = True) -&gt; Iterator[torch.nn.parameter.Parameter]</dt><dd><tt>Returns&nbsp;an&nbsp;iterator&nbsp;over&nbsp;module&nbsp;parameters.<br>
&nbsp;<br>
This&nbsp;is&nbsp;typically&nbsp;passed&nbsp;to&nbsp;an&nbsp;optimizer.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;recurse&nbsp;(bool):&nbsp;if&nbsp;True,&nbsp;then&nbsp;yields&nbsp;parameters&nbsp;of&nbsp;this&nbsp;module<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;and&nbsp;all&nbsp;submodules.&nbsp;Otherwise,&nbsp;yields&nbsp;only&nbsp;parameters&nbsp;that<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;are&nbsp;direct&nbsp;members&nbsp;of&nbsp;this&nbsp;module.<br>
&nbsp;<br>
Yields:<br>
&nbsp;&nbsp;&nbsp;&nbsp;Parameter:&nbsp;module&nbsp;parameter<br>
&nbsp;<br>
Example::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;for&nbsp;param&nbsp;in&nbsp;model.<a href="#MLP-parameters">parameters</a>():<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(<a href="#MLP-type">type</a>(param),&nbsp;param.size())<br>
&nbsp;&nbsp;&nbsp;&nbsp;&lt;class&nbsp;'torch.Tensor'&gt;&nbsp;(20L,)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&lt;class&nbsp;'torch.Tensor'&gt;&nbsp;(20L,&nbsp;1L,&nbsp;5L,&nbsp;5L)</tt></dd></dl>

<dl><dt><a name="MLP-register_backward_hook"><strong>register_backward_hook</strong></a>(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -&gt; torch.utils.hooks.RemovableHandle</dt><dd><tt>Registers&nbsp;a&nbsp;backward&nbsp;hook&nbsp;on&nbsp;the&nbsp;module.<br>
&nbsp;<br>
..&nbsp;warning&nbsp;::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;current&nbsp;implementation&nbsp;will&nbsp;not&nbsp;have&nbsp;the&nbsp;presented&nbsp;behavior<br>
&nbsp;&nbsp;&nbsp;&nbsp;for&nbsp;complex&nbsp;:class:`<a href="torch.nn.modules.module.html#Module">Module</a>`&nbsp;that&nbsp;perform&nbsp;many&nbsp;operations.<br>
&nbsp;&nbsp;&nbsp;&nbsp;In&nbsp;some&nbsp;failure&nbsp;cases,&nbsp;:attr:`grad_input`&nbsp;and&nbsp;:attr:`grad_output`&nbsp;will&nbsp;only<br>
&nbsp;&nbsp;&nbsp;&nbsp;contain&nbsp;the&nbsp;gradients&nbsp;for&nbsp;a&nbsp;subset&nbsp;of&nbsp;the&nbsp;inputs&nbsp;and&nbsp;outputs.<br>
&nbsp;&nbsp;&nbsp;&nbsp;For&nbsp;such&nbsp;:class:`<a href="torch.nn.modules.module.html#Module">Module</a>`,&nbsp;you&nbsp;should&nbsp;use&nbsp;:func:`torch.Tensor.register_hook`<br>
&nbsp;&nbsp;&nbsp;&nbsp;directly&nbsp;on&nbsp;a&nbsp;specific&nbsp;input&nbsp;or&nbsp;output&nbsp;to&nbsp;get&nbsp;the&nbsp;required&nbsp;gradients.<br>
&nbsp;<br>
The&nbsp;hook&nbsp;will&nbsp;be&nbsp;called&nbsp;every&nbsp;time&nbsp;the&nbsp;gradients&nbsp;with&nbsp;respect&nbsp;to&nbsp;module<br>
inputs&nbsp;are&nbsp;computed.&nbsp;The&nbsp;hook&nbsp;should&nbsp;have&nbsp;the&nbsp;following&nbsp;signature::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;hook(module,&nbsp;grad_input,&nbsp;grad_output)&nbsp;-&gt;&nbsp;Tensor&nbsp;or&nbsp;None<br>
&nbsp;<br>
The&nbsp;:attr:`grad_input`&nbsp;and&nbsp;:attr:`grad_output`&nbsp;may&nbsp;be&nbsp;tuples&nbsp;if&nbsp;the<br>
module&nbsp;has&nbsp;multiple&nbsp;inputs&nbsp;or&nbsp;outputs.&nbsp;The&nbsp;hook&nbsp;should&nbsp;not&nbsp;modify&nbsp;its<br>
arguments,&nbsp;but&nbsp;it&nbsp;can&nbsp;optionally&nbsp;return&nbsp;a&nbsp;new&nbsp;gradient&nbsp;with&nbsp;respect&nbsp;to<br>
input&nbsp;that&nbsp;will&nbsp;be&nbsp;used&nbsp;in&nbsp;place&nbsp;of&nbsp;:attr:`grad_input`&nbsp;in&nbsp;subsequent<br>
computations.&nbsp;:attr:`grad_input`&nbsp;will&nbsp;only&nbsp;correspond&nbsp;to&nbsp;the&nbsp;inputs&nbsp;given<br>
as&nbsp;positional&nbsp;arguments.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;:class:`torch.utils.hooks.RemovableHandle`:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;a&nbsp;handle&nbsp;that&nbsp;can&nbsp;be&nbsp;used&nbsp;to&nbsp;remove&nbsp;the&nbsp;added&nbsp;hook&nbsp;by&nbsp;calling<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;``handle.remove()``</tt></dd></dl>

<dl><dt><a name="MLP-register_buffer"><strong>register_buffer</strong></a>(self, name: str, tensor: Union[torch.Tensor, NoneType], persistent: bool = True) -&gt; None</dt><dd><tt>Adds&nbsp;a&nbsp;buffer&nbsp;to&nbsp;the&nbsp;module.<br>
&nbsp;<br>
This&nbsp;is&nbsp;typically&nbsp;used&nbsp;to&nbsp;register&nbsp;a&nbsp;buffer&nbsp;that&nbsp;should&nbsp;not&nbsp;to&nbsp;be<br>
considered&nbsp;a&nbsp;model&nbsp;parameter.&nbsp;For&nbsp;example,&nbsp;BatchNorm's&nbsp;``running_mean``<br>
is&nbsp;not&nbsp;a&nbsp;parameter,&nbsp;but&nbsp;is&nbsp;part&nbsp;of&nbsp;the&nbsp;module's&nbsp;state.&nbsp;Buffers,&nbsp;by<br>
default,&nbsp;are&nbsp;persistent&nbsp;and&nbsp;will&nbsp;be&nbsp;saved&nbsp;alongside&nbsp;parameters.&nbsp;This<br>
behavior&nbsp;can&nbsp;be&nbsp;changed&nbsp;by&nbsp;setting&nbsp;:attr:`persistent`&nbsp;to&nbsp;``False``.&nbsp;The<br>
only&nbsp;difference&nbsp;between&nbsp;a&nbsp;persistent&nbsp;buffer&nbsp;and&nbsp;a&nbsp;non-persistent&nbsp;buffer<br>
is&nbsp;that&nbsp;the&nbsp;latter&nbsp;will&nbsp;not&nbsp;be&nbsp;a&nbsp;part&nbsp;of&nbsp;this&nbsp;module's<br>
:attr:`state_dict`.<br>
&nbsp;<br>
Buffers&nbsp;can&nbsp;be&nbsp;accessed&nbsp;as&nbsp;attributes&nbsp;using&nbsp;given&nbsp;names.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;name&nbsp;(string):&nbsp;name&nbsp;of&nbsp;the&nbsp;buffer.&nbsp;The&nbsp;buffer&nbsp;can&nbsp;be&nbsp;accessed<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;this&nbsp;module&nbsp;using&nbsp;the&nbsp;given&nbsp;name<br>
&nbsp;&nbsp;&nbsp;&nbsp;tensor&nbsp;(Tensor):&nbsp;buffer&nbsp;to&nbsp;be&nbsp;registered.<br>
&nbsp;&nbsp;&nbsp;&nbsp;persistent&nbsp;(bool):&nbsp;whether&nbsp;the&nbsp;buffer&nbsp;is&nbsp;part&nbsp;of&nbsp;this&nbsp;module's<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;:attr:`state_dict`.<br>
&nbsp;<br>
Example::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;self.<a href="#MLP-register_buffer">register_buffer</a>('running_mean',&nbsp;torch.zeros(num_features))</tt></dd></dl>

<dl><dt><a name="MLP-register_forward_hook"><strong>register_forward_hook</strong></a>(self, hook: Callable[..., NoneType]) -&gt; torch.utils.hooks.RemovableHandle</dt><dd><tt>Registers&nbsp;a&nbsp;forward&nbsp;hook&nbsp;on&nbsp;the&nbsp;module.<br>
&nbsp;<br>
The&nbsp;hook&nbsp;will&nbsp;be&nbsp;called&nbsp;every&nbsp;time&nbsp;after&nbsp;:func:`forward`&nbsp;has&nbsp;computed&nbsp;an&nbsp;output.<br>
It&nbsp;should&nbsp;have&nbsp;the&nbsp;following&nbsp;signature::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;hook(module,&nbsp;input,&nbsp;output)&nbsp;-&gt;&nbsp;None&nbsp;or&nbsp;modified&nbsp;output<br>
&nbsp;<br>
The&nbsp;input&nbsp;contains&nbsp;only&nbsp;the&nbsp;positional&nbsp;arguments&nbsp;given&nbsp;to&nbsp;the&nbsp;module.<br>
Keyword&nbsp;arguments&nbsp;won't&nbsp;be&nbsp;passed&nbsp;to&nbsp;the&nbsp;hooks&nbsp;and&nbsp;only&nbsp;to&nbsp;the&nbsp;``forward``.<br>
The&nbsp;hook&nbsp;can&nbsp;modify&nbsp;the&nbsp;output.&nbsp;It&nbsp;can&nbsp;modify&nbsp;the&nbsp;input&nbsp;inplace&nbsp;but<br>
it&nbsp;will&nbsp;not&nbsp;have&nbsp;effect&nbsp;on&nbsp;forward&nbsp;since&nbsp;this&nbsp;is&nbsp;called&nbsp;after<br>
:func:`forward`&nbsp;is&nbsp;called.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;:class:`torch.utils.hooks.RemovableHandle`:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;a&nbsp;handle&nbsp;that&nbsp;can&nbsp;be&nbsp;used&nbsp;to&nbsp;remove&nbsp;the&nbsp;added&nbsp;hook&nbsp;by&nbsp;calling<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;``handle.remove()``</tt></dd></dl>

<dl><dt><a name="MLP-register_forward_pre_hook"><strong>register_forward_pre_hook</strong></a>(self, hook: Callable[..., NoneType]) -&gt; torch.utils.hooks.RemovableHandle</dt><dd><tt>Registers&nbsp;a&nbsp;forward&nbsp;pre-hook&nbsp;on&nbsp;the&nbsp;module.<br>
&nbsp;<br>
The&nbsp;hook&nbsp;will&nbsp;be&nbsp;called&nbsp;every&nbsp;time&nbsp;before&nbsp;:func:`forward`&nbsp;is&nbsp;invoked.<br>
It&nbsp;should&nbsp;have&nbsp;the&nbsp;following&nbsp;signature::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;hook(module,&nbsp;input)&nbsp;-&gt;&nbsp;None&nbsp;or&nbsp;modified&nbsp;input<br>
&nbsp;<br>
The&nbsp;input&nbsp;contains&nbsp;only&nbsp;the&nbsp;positional&nbsp;arguments&nbsp;given&nbsp;to&nbsp;the&nbsp;module.<br>
Keyword&nbsp;arguments&nbsp;won't&nbsp;be&nbsp;passed&nbsp;to&nbsp;the&nbsp;hooks&nbsp;and&nbsp;only&nbsp;to&nbsp;the&nbsp;``forward``.<br>
The&nbsp;hook&nbsp;can&nbsp;modify&nbsp;the&nbsp;input.&nbsp;User&nbsp;can&nbsp;either&nbsp;return&nbsp;a&nbsp;tuple&nbsp;or&nbsp;a<br>
single&nbsp;modified&nbsp;value&nbsp;in&nbsp;the&nbsp;hook.&nbsp;We&nbsp;will&nbsp;wrap&nbsp;the&nbsp;value&nbsp;into&nbsp;a&nbsp;tuple<br>
if&nbsp;a&nbsp;single&nbsp;value&nbsp;is&nbsp;returned(unless&nbsp;that&nbsp;value&nbsp;is&nbsp;already&nbsp;a&nbsp;tuple).<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;:class:`torch.utils.hooks.RemovableHandle`:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;a&nbsp;handle&nbsp;that&nbsp;can&nbsp;be&nbsp;used&nbsp;to&nbsp;remove&nbsp;the&nbsp;added&nbsp;hook&nbsp;by&nbsp;calling<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;``handle.remove()``</tt></dd></dl>

<dl><dt><a name="MLP-register_parameter"><strong>register_parameter</strong></a>(self, name: str, param: Union[torch.nn.parameter.Parameter, NoneType]) -&gt; None</dt><dd><tt>Adds&nbsp;a&nbsp;parameter&nbsp;to&nbsp;the&nbsp;module.<br>
&nbsp;<br>
The&nbsp;parameter&nbsp;can&nbsp;be&nbsp;accessed&nbsp;as&nbsp;an&nbsp;attribute&nbsp;using&nbsp;given&nbsp;name.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;name&nbsp;(string):&nbsp;name&nbsp;of&nbsp;the&nbsp;parameter.&nbsp;The&nbsp;parameter&nbsp;can&nbsp;be&nbsp;accessed<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;this&nbsp;module&nbsp;using&nbsp;the&nbsp;given&nbsp;name<br>
&nbsp;&nbsp;&nbsp;&nbsp;param&nbsp;(Parameter):&nbsp;parameter&nbsp;to&nbsp;be&nbsp;added&nbsp;to&nbsp;the&nbsp;module.</tt></dd></dl>

<dl><dt><a name="MLP-requires_grad_"><strong>requires_grad_</strong></a>(self: ~T, requires_grad: bool = True) -&gt; ~T</dt><dd><tt>Change&nbsp;if&nbsp;autograd&nbsp;should&nbsp;record&nbsp;operations&nbsp;on&nbsp;parameters&nbsp;in&nbsp;this<br>
module.<br>
&nbsp;<br>
This&nbsp;method&nbsp;sets&nbsp;the&nbsp;parameters'&nbsp;:attr:`requires_grad`&nbsp;attributes<br>
in-place.<br>
&nbsp;<br>
This&nbsp;method&nbsp;is&nbsp;helpful&nbsp;for&nbsp;freezing&nbsp;part&nbsp;of&nbsp;the&nbsp;module&nbsp;for&nbsp;finetuning<br>
or&nbsp;training&nbsp;parts&nbsp;of&nbsp;a&nbsp;model&nbsp;individually&nbsp;(e.g.,&nbsp;GAN&nbsp;training).<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;requires_grad&nbsp;(bool):&nbsp;whether&nbsp;autograd&nbsp;should&nbsp;record&nbsp;operations&nbsp;on<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;parameters&nbsp;in&nbsp;this&nbsp;module.&nbsp;Default:&nbsp;``True``.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</tt></dd></dl>

<dl><dt><a name="MLP-share_memory"><strong>share_memory</strong></a>(self: ~T) -&gt; ~T</dt></dl>

<dl><dt><a name="MLP-state_dict"><strong>state_dict</strong></a>(self, destination=None, prefix='', keep_vars=False)</dt><dd><tt>Returns&nbsp;a&nbsp;dictionary&nbsp;containing&nbsp;a&nbsp;whole&nbsp;state&nbsp;of&nbsp;the&nbsp;module.<br>
&nbsp;<br>
Both&nbsp;parameters&nbsp;and&nbsp;persistent&nbsp;buffers&nbsp;(e.g.&nbsp;running&nbsp;averages)&nbsp;are<br>
included.&nbsp;Keys&nbsp;are&nbsp;corresponding&nbsp;parameter&nbsp;and&nbsp;buffer&nbsp;names.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;dict:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;a&nbsp;dictionary&nbsp;containing&nbsp;a&nbsp;whole&nbsp;state&nbsp;of&nbsp;the&nbsp;module<br>
&nbsp;<br>
Example::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;module.<a href="#MLP-state_dict">state_dict</a>().keys()<br>
&nbsp;&nbsp;&nbsp;&nbsp;['bias',&nbsp;'weight']</tt></dd></dl>

<dl><dt><a name="MLP-to"><strong>to</strong></a>(self, *args, **kwargs)</dt><dd><tt>Moves&nbsp;and/or&nbsp;casts&nbsp;the&nbsp;parameters&nbsp;and&nbsp;buffers.<br>
&nbsp;<br>
This&nbsp;can&nbsp;be&nbsp;called&nbsp;as<br>
&nbsp;<br>
..&nbsp;function::&nbsp;<a href="#MLP-to">to</a>(device=None,&nbsp;dtype=None,&nbsp;non_blocking=False)<br>
&nbsp;<br>
..&nbsp;function::&nbsp;<a href="#MLP-to">to</a>(dtype,&nbsp;non_blocking=False)<br>
&nbsp;<br>
..&nbsp;function::&nbsp;<a href="#MLP-to">to</a>(tensor,&nbsp;non_blocking=False)<br>
&nbsp;<br>
..&nbsp;function::&nbsp;<a href="#MLP-to">to</a>(memory_format=torch.channels_last)<br>
&nbsp;<br>
Its&nbsp;signature&nbsp;is&nbsp;similar&nbsp;to&nbsp;:meth:`torch.Tensor.to`,&nbsp;but&nbsp;only&nbsp;accepts<br>
floating&nbsp;point&nbsp;desired&nbsp;:attr:`dtype`&nbsp;s.&nbsp;In&nbsp;addition,&nbsp;this&nbsp;method&nbsp;will<br>
only&nbsp;cast&nbsp;the&nbsp;floating&nbsp;point&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;to&nbsp;:attr:`dtype`<br>
(if&nbsp;given).&nbsp;The&nbsp;integral&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;will&nbsp;be&nbsp;moved<br>
:attr:`device`,&nbsp;if&nbsp;that&nbsp;is&nbsp;given,&nbsp;but&nbsp;with&nbsp;dtypes&nbsp;unchanged.&nbsp;When<br>
:attr:`non_blocking`&nbsp;is&nbsp;set,&nbsp;it&nbsp;tries&nbsp;to&nbsp;convert/move&nbsp;asynchronously<br>
with&nbsp;respect&nbsp;to&nbsp;the&nbsp;host&nbsp;if&nbsp;possible,&nbsp;e.g.,&nbsp;moving&nbsp;CPU&nbsp;Tensors&nbsp;with<br>
pinned&nbsp;memory&nbsp;to&nbsp;CUDA&nbsp;devices.<br>
&nbsp;<br>
See&nbsp;below&nbsp;for&nbsp;examples.<br>
&nbsp;<br>
..&nbsp;note::<br>
&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;method&nbsp;modifies&nbsp;the&nbsp;module&nbsp;in-place.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;device&nbsp;(:class:`torch.device`):&nbsp;the&nbsp;desired&nbsp;device&nbsp;of&nbsp;the&nbsp;parameters<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;and&nbsp;buffers&nbsp;in&nbsp;this&nbsp;module<br>
&nbsp;&nbsp;&nbsp;&nbsp;dtype&nbsp;(:class:`torch.dtype`):&nbsp;the&nbsp;desired&nbsp;floating&nbsp;point&nbsp;type&nbsp;of<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;floating&nbsp;point&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;in&nbsp;this&nbsp;module<br>
&nbsp;&nbsp;&nbsp;&nbsp;tensor&nbsp;(torch.Tensor):&nbsp;Tensor&nbsp;whose&nbsp;dtype&nbsp;and&nbsp;device&nbsp;are&nbsp;the&nbsp;desired<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;dtype&nbsp;and&nbsp;device&nbsp;for&nbsp;all&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;in&nbsp;this&nbsp;module<br>
&nbsp;&nbsp;&nbsp;&nbsp;memory_format&nbsp;(:class:`torch.memory_format`):&nbsp;the&nbsp;desired&nbsp;memory<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;format&nbsp;for&nbsp;4D&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;in&nbsp;this&nbsp;module&nbsp;(keyword<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;only&nbsp;argument)<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self<br>
&nbsp;<br>
Example::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;linear&nbsp;=&nbsp;nn.Linear(2,&nbsp;2)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;linear.weight<br>
&nbsp;&nbsp;&nbsp;&nbsp;Parameter&nbsp;containing:<br>
&nbsp;&nbsp;&nbsp;&nbsp;tensor([[&nbsp;0.1913,&nbsp;-0.3420],<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[-0.5113,&nbsp;-0.2325]])<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;linear.<a href="#MLP-to">to</a>(torch.double)<br>
&nbsp;&nbsp;&nbsp;&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;linear.weight<br>
&nbsp;&nbsp;&nbsp;&nbsp;Parameter&nbsp;containing:<br>
&nbsp;&nbsp;&nbsp;&nbsp;tensor([[&nbsp;0.1913,&nbsp;-0.3420],<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[-0.5113,&nbsp;-0.2325]],&nbsp;dtype=torch.float64)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;gpu1&nbsp;=&nbsp;torch.device("cuda:1")<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;linear.<a href="#MLP-to">to</a>(gpu1,&nbsp;dtype=torch.half,&nbsp;non_blocking=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;linear.weight<br>
&nbsp;&nbsp;&nbsp;&nbsp;Parameter&nbsp;containing:<br>
&nbsp;&nbsp;&nbsp;&nbsp;tensor([[&nbsp;0.1914,&nbsp;-0.3420],<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[-0.5112,&nbsp;-0.2324]],&nbsp;dtype=torch.float16,&nbsp;device='cuda:1')<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;cpu&nbsp;=&nbsp;torch.device("cpu")<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;linear.<a href="#MLP-to">to</a>(cpu)<br>
&nbsp;&nbsp;&nbsp;&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;linear.weight<br>
&nbsp;&nbsp;&nbsp;&nbsp;Parameter&nbsp;containing:<br>
&nbsp;&nbsp;&nbsp;&nbsp;tensor([[&nbsp;0.1914,&nbsp;-0.3420],<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[-0.5112,&nbsp;-0.2324]],&nbsp;dtype=torch.float16)</tt></dd></dl>

<dl><dt><a name="MLP-train"><strong>train</strong></a>(self: ~T, mode: bool = True) -&gt; ~T</dt><dd><tt>Sets&nbsp;the&nbsp;module&nbsp;in&nbsp;training&nbsp;mode.<br>
&nbsp;<br>
This&nbsp;has&nbsp;any&nbsp;effect&nbsp;only&nbsp;on&nbsp;certain&nbsp;modules.&nbsp;See&nbsp;documentations&nbsp;of<br>
particular&nbsp;modules&nbsp;for&nbsp;details&nbsp;of&nbsp;their&nbsp;behaviors&nbsp;in&nbsp;training/evaluation<br>
mode,&nbsp;if&nbsp;they&nbsp;are&nbsp;affected,&nbsp;e.g.&nbsp;:class:`Dropout`,&nbsp;:class:`BatchNorm`,<br>
etc.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;mode&nbsp;(bool):&nbsp;whether&nbsp;to&nbsp;set&nbsp;training&nbsp;mode&nbsp;(``True``)&nbsp;or&nbsp;evaluation<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;mode&nbsp;(``False``).&nbsp;Default:&nbsp;``True``.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</tt></dd></dl>

<dl><dt><a name="MLP-type"><strong>type</strong></a>(self: ~T, dst_type: Union[torch.dtype, str]) -&gt; ~T</dt><dd><tt>Casts&nbsp;all&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;to&nbsp;:attr:`dst_type`.<br>
&nbsp;<br>
Arguments:<br>
&nbsp;&nbsp;&nbsp;&nbsp;dst_type&nbsp;(type&nbsp;or&nbsp;string):&nbsp;the&nbsp;desired&nbsp;type<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</tt></dd></dl>

<dl><dt><a name="MLP-zero_grad"><strong>zero_grad</strong></a>(self, set_to_none: bool = False) -&gt; None</dt><dd><tt>Sets&nbsp;gradients&nbsp;of&nbsp;all&nbsp;model&nbsp;parameters&nbsp;to&nbsp;zero.&nbsp;See&nbsp;similar&nbsp;function<br>
under&nbsp;:class:`torch.optim.Optimizer`&nbsp;for&nbsp;more&nbsp;context.<br>
&nbsp;<br>
Arguments:<br>
&nbsp;&nbsp;&nbsp;&nbsp;set_to_none&nbsp;(bool):&nbsp;instead&nbsp;of&nbsp;setting&nbsp;to&nbsp;zero,&nbsp;set&nbsp;the&nbsp;grads&nbsp;to&nbsp;None.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;See&nbsp;:meth:`torch.optim.Optimizer.zero_grad`&nbsp;for&nbsp;details.</tt></dd></dl>

<hr>
Data descriptors inherited from <a href="torch.nn.modules.module.html#Module">torch.nn.modules.module.Module</a>:<br>
<dl><dt><strong>__dict__</strong></dt>
<dd><tt>dictionary&nbsp;for&nbsp;instance&nbsp;variables&nbsp;(if&nbsp;defined)</tt></dd>
</dl>
<dl><dt><strong>__weakref__</strong></dt>
<dd><tt>list&nbsp;of&nbsp;weak&nbsp;references&nbsp;to&nbsp;the&nbsp;object&nbsp;(if&nbsp;defined)</tt></dd>
</dl>
<hr>
Data and other attributes inherited from <a href="torch.nn.modules.module.html#Module">torch.nn.modules.module.Module</a>:<br>
<dl><dt><strong>T_destination</strong> = ~T_destination</dl>

<dl><dt><strong>__annotations__</strong> = {'__call__': typing.Callable[..., typing.Any], '_version': &lt;class 'int'&gt;, 'dump_patches': &lt;class 'bool'&gt;, 'forward': typing.Callable[..., typing.Any], 'training': &lt;class 'bool'&gt;}</dl>

<dl><dt><strong>dump_patches</strong> = False</dl>

</td></tr></table></td></tr></table><p>
<table width="100%" cellspacing=0 cellpadding=2 border=0 summary="section">
<tr bgcolor="#eeaa77">
<td colspan=3 valign=bottom>&nbsp;<br>
<font color="#ffffff" face="helvetica, arial"><big><strong>Functions</strong></big></font></td></tr>
    
<tr><td bgcolor="#eeaa77"><tt>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</tt></td><td>&nbsp;</td>
<td width="100%"><dl><dt><a name="-evaluate_model"><strong>evaluate_model</strong></a>(test_dl, model)</dt><dd><tt>After&nbsp;we&nbsp;have&nbsp;trained&nbsp;our&nbsp;model,&nbsp;we&nbsp;are&nbsp;calculating&nbsp;the&nbsp;accuracy&nbsp;of<br>
trained&nbsp;model&nbsp;on&nbsp;the&nbsp;test&nbsp;dataset&nbsp;-&nbsp;percentage&nbsp;of&nbsp;samples&nbsp;that&nbsp;are<br>
classified&nbsp;correctly.<br>
&nbsp;<br>
Parameters<br>
----------<br>
test_dl&nbsp;:&nbsp;test&nbsp;dataset<br>
model&nbsp;:&nbsp;object&nbsp;of&nbsp;<a href="#MLP">MLP</a>&nbsp;class<br>
&nbsp;<br>
Returns<br>
-------<br>
acc&nbsp;:&nbsp;model&nbsp;accuracy&nbsp;on&nbsp;test&nbsp;dataset</tt></dd></dl>
 <dl><dt><a name="-get_last_layer"><strong>get_last_layer</strong></a>(data, model)</dt><dd><tt>For&nbsp;implementing&nbsp;the&nbsp;second&nbsp;pipeline,&nbsp;we&nbsp;are&nbsp;using&nbsp;logistic&nbsp;regression<br>
as&nbsp;helper&nbsp;classifier.&nbsp;We&nbsp;are&nbsp;extracting&nbsp;activations&nbsp;from&nbsp;the&nbsp;last&nbsp;hidden<br>
layer&nbsp;and&nbsp;feed&nbsp;them&nbsp;into&nbsp;the&nbsp;helper&nbsp;classifier&nbsp;to&nbsp;predict&nbsp;the&nbsp;original<br>
task.&nbsp;This&nbsp;function&nbsp;returns&nbsp;the&nbsp;activations&nbsp;from&nbsp;the&nbsp;last&nbsp;hidden&nbsp;layer.<br>
We&nbsp;keep&nbsp;track&nbsp;on&nbsp;the&nbsp;inputs&nbsp;for&nbsp;which&nbsp;we&nbsp;extract&nbsp;activations,&nbsp;along&nbsp;with<br>
the&nbsp;target(original)&nbsp;labels.<br>
&nbsp;<br>
Parameters<br>
----------<br>
data&nbsp;:&nbsp;training&nbsp;dataset<br>
model&nbsp;:&nbsp;object&nbsp;of&nbsp;<a href="#MLP">MLP</a>&nbsp;class<br>
&nbsp;<br>
Returns<br>
-------<br>
xinputs&nbsp;:&nbsp;activations&nbsp;from&nbsp;the&nbsp;last&nbsp;hidden&nbsp;layer<br>
oinputs&nbsp;:&nbsp;inputs&nbsp;in&nbsp;the&nbsp;order&nbsp;in&nbsp;which&nbsp;we&nbsp;calculate&nbsp;activations<br>
true&nbsp;:&nbsp;original&nbsp;(true)&nbsp;target&nbsp;values</tt></dd></dl>
 <dl><dt><a name="-get_soft_labels"><strong>get_soft_labels</strong></a>(data, model)</dt><dd><tt>For&nbsp;implementing&nbsp;the&nbsp;first&nbsp;pipeline,&nbsp;we&nbsp;are&nbsp;using&nbsp;predicted&nbsp;soft&nbsp;labels<br>
for&nbsp;GBT&nbsp;training.&nbsp;This&nbsp;function&nbsp;returns&nbsp;predicted&nbsp;soft&nbsp;labels,&nbsp;along<br>
with&nbsp;inputs,&nbsp;we&nbsp;keep&nbsp;track&nbsp;on&nbsp;the&nbsp;inputs&nbsp;for&nbsp;which&nbsp;we&nbsp;make&nbsp;predictions,<br>
along&nbsp;with&nbsp;the&nbsp;target(original)&nbsp;labels.<br>
Parameters<br>
----------<br>
data&nbsp;:&nbsp;training&nbsp;dataset<br>
model&nbsp;:&nbsp;object&nbsp;of&nbsp;<a href="#MLP">MLP</a>&nbsp;class<br>
Returns<br>
-------<br>
xinputs&nbsp;:&nbsp;inputs&nbsp;in&nbsp;the&nbsp;order&nbsp;in&nbsp;which&nbsp;we&nbsp;calculate&nbsp;predictions<br>
predictions&nbsp;:&nbsp;soft&nbsp;labels,&nbsp;without&nbsp;rounding<br>
true&nbsp;:&nbsp;original&nbsp;(true)&nbsp;target&nbsp;values</tt></dd></dl>
 <dl><dt><a name="-train_model"><strong>train_model</strong></a>(train_dl, test_dl, model)</dt><dd><tt>For&nbsp;training&nbsp;of&nbsp;defined&nbsp;<a href="#MLP">MLP</a>&nbsp;model,&nbsp;we&nbsp;have&nbsp;to&nbsp;define&nbsp;loss&nbsp;function<br>
and&nbsp;optimization&nbsp;algorithm&nbsp;that&nbsp;will&nbsp;be&nbsp;used.&nbsp;Binary&nbsp;cross&nbsp;entropy<br>
loss&nbsp;is&nbsp;used&nbsp;as&nbsp;loss&nbsp;function.&nbsp;Stochastic&nbsp;gradient&nbsp;descent&nbsp;is&nbsp;used<br>
for&nbsp;optimization.&nbsp;SGD&nbsp;class&nbsp;provides&nbsp;standard&nbsp;algorithm.&nbsp;In&nbsp;the&nbsp;outer<br>
loop,&nbsp;we&nbsp;are&nbsp;defining&nbsp;the&nbsp;number&nbsp;of&nbsp;training&nbsp;epochs.&nbsp;In&nbsp;each&nbsp;epoch,<br>
the&nbsp;inner&nbsp;loop&nbsp;is&nbsp;required&nbsp;for&nbsp;enumerating&nbsp;the&nbsp;mini&nbsp;batches&nbsp;for&nbsp;SGD.<br>
Each&nbsp;update&nbsp;of&nbsp;the&nbsp;model&nbsp;consists&nbsp;of&nbsp;the&nbsp;following&nbsp;steps:&nbsp;clear&nbsp;the<br>
gradients,&nbsp;feed&nbsp;the&nbsp;inputs&nbsp;to&nbsp;the&nbsp;network,&nbsp;calculate&nbsp;loss,&nbsp;backpropagate<br>
the&nbsp;error&nbsp;through&nbsp;the&nbsp;network,&nbsp;update&nbsp;model&nbsp;weights.Additionaly,&nbsp;this<br>
function&nbsp;plots&nbsp;training&nbsp;and&nbsp;validation&nbsp;learning&nbsp;curves.<br>
&nbsp;<br>
Parameters<br>
----------<br>
train_dl&nbsp;:&nbsp;training&nbsp;dataset<br>
test_dl&nbsp;:&nbsp;test&nbsp;dataset<br>
model&nbsp;:&nbsp;object&nbsp;of&nbsp;<a href="#MLP">MLP</a>&nbsp;class<br>
&nbsp;<br>
Returns<br>
-------<br>
None.</tt></dd></dl>
</td></tr></table>
</body></html>